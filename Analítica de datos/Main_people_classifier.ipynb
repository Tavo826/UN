{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit ('FG': virtualenv)",
      "language": "python",
      "name": "python37664bitfgvirtualenv5dc1badde07b4911ac66d98ef2dee124"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Main_people_classifier.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tavo826/UN/blob/main/Main_people_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPcmUFK0duc-"
      },
      "source": [
        "# Gaussian Naive Bayes\n",
        "\n",
        "## sklearn.naive_bayes.GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
        "\n",
        "Puede realizar actualizaciones en línea de los parámetros del modelo a través de partial_fit\n",
        "\n",
        "Se basa en el supuesto \"ingenuo\" (naive) de independencia condicional entre cada par de características dado el valor de la variable de la clase. El teorema de Bayes establece la siguiente relación\n",
        "\n",
        "$$P(y|x_1,...,x_n) = \\frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$\n",
        "\n",
        "Donde $y$ es la variable de la clase y x el vector de características.\n",
        "\n",
        "Usando la suposición de independencia condicional ingenua\n",
        "\n",
        "$$P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_n) = P(x_i|y)$$\n",
        "\n",
        "Para todos los i esto se simplifica a\n",
        "\n",
        "$$P(y|x_1,...,x_n) = \\frac{P(y)\\prod_{i=1}^{n}P(x_i|y)}{P(x_1,...,x_n)}$$\n",
        "\n",
        "\n",
        "Como $P(x_1,...,x_n)$ es constante, se puede considerar:\n",
        "\n",
        "$$P(y|x_1,...,x_n) \\propto P(y)\\prod_{i=1}^{n}P(x_i|y)$$\n",
        "\n",
        "Por lo tanto:\n",
        "\n",
        "$$\\hat{y} = \\underset{y}{argmax} P(y)\\prod_{i=1}^{n}P(x_i|y)$$\n",
        "\n",
        "Y se puede usar la estimación del máximo A posteriori, para estimar $P(y)$ y $P(x_i|y)$.\n",
        "\n",
        "Los diferentes clasificadores ingenuos de Bayes difieren principalmente por los supuestos que hacen con respecto a la distribución de $P(x_i|y)$\n",
        "\n",
        "El Gaussian Naive Bayes supone que $P(x_i|y)$ es gaussiana\n",
        "\n",
        "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi{{\\sigma}_{y}^{2}}}} exp \\left( -\\frac{{(x_i-{\\mu}_{y})}^{2}}{2{{\\sigma}_{y}^{2}}} \\right)$$\n",
        "\n",
        "Los parámetros ${\\sigma}_{y}$ y ${\\mu}_{y}$ se estiman usando la máxima verosimilitud.\n",
        "\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "\n",
        "## sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
        "\n",
        "Estrictamente hablando, SGD es simplemente una técnica de optimización y no corresponde a una familia específica de modelos de aprendizaje automático. Es solo una forma de entrenar a un modelo. A menudo, una instancia de SGDClassifier o SGDRegressor tendrá un estimador equivalente en la API scikit-learn, posiblemente utilizando una técnica de optimización diferente. Por ejemplo, el uso de SGDClassifier (loss = 'log') da como resultado una regresión logística, es decir, un modelo equivalente a LogisticRegression que se ajusta a través de SGD en lugar de ser ajustado por uno de los otros solucionadores en LogisticRegression. \n",
        "\n",
        "La clase SGDClassifier implementa una rutina de aprendizaje de descenso de gradiente estocástico simple que admite diferentes funciones de pérdida y penalizaciones para la clasificación.\n",
        "\n",
        "El objetivo de este clasificador es aprender una fución de puntuación lineal $f(x) = w^Tx+b$ para hacer predicciones para realizar clasificación binaria. Para encontrar los parámetros del modelo, se minimiza el error de entrenamiento regularizado dado por: $$\\theta^* = \\frac{1}{n}\\sum_{i=1}^{n}L(y_i,f(x_i)) + \\alpha R(\\omega)$$ donde **L** es una función de error que mide el ajuste del modelo y **R** es el término de regularización que penaliza la complejidad del modelo; **$\\alpha > 0$** es un hiperparámetro que controla la fuerza de regularización.\n",
        "\n",
        "El clasificador admite diferentes funciones de pérdida:\n",
        "\n",
        "* Hinge: (soft-margin) linear Support Vector Machine\n",
        "\n",
        "    Perezosa - Actualiza los parámetros de un modelo si un ejemplo viola la restricción de margen, lo que hace que el entrenamiento sea muy eficiente y puede dar como resultado modelos más dispersos (coeficientes 0), incluso usando la penalización $\\ell_2$ $$L(y_i,f(x_i)) = max(0, 1 - y_if(x_i))$$\n",
        "\n",
        "* Perceptron: $$L(y_i,f(x_i)) = max(0, - y_if(x_i))$$\n",
        "\n",
        "* Modified Huber: smoothed hinge loss (Perezosa)\n",
        "    \n",
        "    $$L(y_i,f(x_i)) = \\begin{cases} {max(0, 1-y_if(x_i))}^{2} \\quad si \\quad y_if(x_i) > 1\\\\\n",
        "    -4y_if(x_i) \\quad otherwise \\end{cases}$$\n",
        "    \n",
        "* Log: logistic regression $$L(y_i,f(x_i)) = log(1 + exp(-y_if(x_i)))$$\n",
        "\n",
        "* Least-Squares: Linear regression (Ridge o Lasso dependiendo del R) $$L(y_i,f(x_i)) = \\frac{1}{2}{y_i-f(x_i)}^{2}$$\n",
        "\n",
        "* Huber: menos sensitivo que Least-Squares a los valores atípicos.\n",
        "\n",
        "    $L(y_i,f(x_i))=\\begin{cases} Least-Squares \\quad si \\quad |y_i - f(x_i)| \\leq \\epsilon \\\\\n",
        "    \\epsilon|y_i-f(x_i)| - \\frac{1}{2}{\\epsilon}^{2} \\quad otherwise \\end{cases}$\n",
        "    \n",
        "* Epsilon-Insensitive: (soft-margin) Support Vector Regression $$L(y_i,f(x_i)) = max(0, |y_i-f(x_i)|-\\epsilon)$$\n",
        "\n",
        "Entre las opciones más escogidas para el término de regularización se encuentran:\n",
        "\n",
        "* Norma ${\\ell}_{2}: R(\\omega) := \\frac{1}{2}\\sum_{j=1}^{m}{\\omega_j}^{2} = {\\|\\omega\\|}_{2}^{2}$\n",
        "* Norma ${\\ell}_{1}: R(\\omega) := \\sum_{j=1}^{m}|\\omega_j|$, lleva a soluciones dispersas\n",
        "* Elastic Net: $R(\\omega) := \\frac{\\rho}{2}\\sum_{j=1}^{n}{\\omega_j}^{2} + (1-\\rho)\\sum_{j=1}^{m}|\\omega_j|$, una combinación convexa de $\\ell_2$ y $\\ell_1$, donde $\\rho$ está dado por 1 - l1_ratio.\n",
        "\n",
        "SGDClassifier admite la clasificación de varias clases mediante la combinación de múltiples clasificadores binarios en un esquema \"uno contra todos\" (OVA). Para cada una de las clases, se aprende un clasificador binario que discrimina entre esa y todas las demás clases. En el momento de la prueba, calculamos el puntaje de confianza (es decir, las distancias firmadas al hiperplano) para cada clasificador y elegimos la clase con la mayor confianza.\n",
        "\n",
        "# Linear Discriminant Analysis\n",
        "\n",
        "## sklearn.discriminant_analysis.LinearDiscriminantAnalysis(*, solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)\n",
        "\n",
        "Se deriva de modelos probabilísticos simples que modelan la distribución condicional de clase de los datos P(X|y). Las predicciones pueden ser obtenidas usando la regla de Bayes y se selecciona la clase que maximice esta probabilidad posterior.\n",
        "\n",
        "Se modela la probabilidad P(x|y)como una distribución Gaussiana multivariada con densidad:\n",
        "\n",
        "$$P(x|y) = \\frac{1}{{(2\\pi)}^{\\frac{n}{2}}{|\\sum_{y}|}^{\\frac{1}{2}}} exp\\left( -\\frac{1}{2}{(x-\\mu_y)}^{T}\\sum_{y}^{-1}(x-\\mu_y) \\right)$$\n",
        "\n",
        "Se asume que las las Gausianas comparten la misma matriz de covarianza. EL log-posterior queda de la forma:\n",
        "\n",
        "$$logP(y|x) = - \\frac{1}{2}{(x-\\mu_y)}^{T}\\sum_{y}^{-1}(x-\\mu_y) + logP(y) + Cst$$\n",
        "\n",
        "El término ${(x-\\mu_y)}^{T}\\sum_{y}^{-1}(x-\\mu_y)$ corresponde a la distancia de Mahalanobis entre la muestra x y la media $\\mu$ de la clase. En LDA se asigna la muestra x a la clase cuya media sea la más cercana en términos de la distancia de Mahalanobis, mientras que cuenta las probabilidades previas de la clase (priors).\n",
        "\n",
        "# Quadratic Discriminant Analysis\n",
        "\n",
        "## sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(*, priors=None, reg_param=0.0, store_covariance=False, tol=0.0001)\n",
        "\n",
        "Se fundamenta de la misma manera que el Linear Discriminant Analysis\n",
        "\n",
        "$$logP(y|x) = logP(x|y) + logP(y) + Cst = -\\frac{1}{2}log\\left|\\sum_y\\right| - \\frac{1}{2}{(x-\\mu_y)}^{T}\\sum_{y}^{-1}(x-\\mu_y) + logP(y) + Cst $$\n",
        "\n",
        "donde la constante Cst corresponde al denominador de la relación de Bayes P(x), adicionado a otras constantes de la Gaussiana. La clase predicha es la que maximiza el log-posterior\n",
        "\n",
        "Si en este modelo se asume que las matrices de covarianza son diagonales, entonces las entradas se asumen condicionalmente independiente en cada clase, y el resultado de clasificador es equivalente al *Gaussian Naive Bayes*\n",
        "\n",
        "# K-Neighbors Classifier\n",
        "\n",
        "## sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
        "\n",
        "La clasificación se calcula a partir de un voto de mayoría simple de los vecinos más cercanos de cada punto: a un punto de consulta se le asigna la clase de datos que tiene la mayor cantidad de representantes dentro de los vecinos más cercanos del punto.\n",
        "\n",
        "KNeighborsClassifier implementa el aprendizaje basado en los k vecinos más cercanos de cada punto de consulta, donde k es un valor entero especificado por el usuario.\n",
        "\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "## sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
        "\n",
        "En el caso de multiclase, el algoritmo de entrenamiento usa el esquema de uno contra todos (OvA) si la opción 'multi_class' está configurada en 'ovr', y usa la pérdida de entropía cruzada si la opción 'multi_class' está configurada en 'multinomial '.\n",
        "\n",
        "Los solucionadores 'newton-cg', 'sag' y 'lbfgs' solo admiten la regularización L2 con formulación primaria, o no regularización. El solucionador \"liblinear\" admite la regularización de L1 y L2, con una formulación dual solo para la penalización de L2. La regularización de Elastic-Net solo es compatible con el solucionador de \"saga\"\n",
        "\n",
        "Usando la penalización $\\ell_2$, se optimiza la función de costo $$\\underset{\\omega,c}{min}\\frac{1}{2}\\omega^T\\omega + C\\sum_{i=1}^{n}log(exp(-y_i(X^T_i\\omega + c))+1)$$\n",
        "\n",
        "De manera similar, usando la penalización $\\ell_2$ se resuelve el problema de optimización $$\\underset{\\omega,c}{min} \\|\\omega\\|_1 + C\\sum_{i=1}^{n}log(exp(-y_i(X^T_i\\omega + c))+1)$$\n",
        "\n",
        "Elastic-Net utiliza una combinación de $\\ell_1$ y $\\ell_2$ y la función de costo es $$\\underset{\\omega,c}{min} \\frac{1-\\rho}{2}\\omega^T\\omega + \\rho\\|\\omega\\|_1 + C\\sum_{i=1}^{n}log(exp(-y_i(X^T_i\\omega + c))+1)$$ donde $\\rho$ controla la regularización.\n",
        "\n",
        "# Support Vector Classifier\n",
        "\n",
        "## sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
        "\n",
        "Una máquina de vectores de soporte construye un hiperplano o un conjunto de hiperplanos en un espacio dimensional alto o infinito, que puede usarse para clasificación, regresión u otras tareas. Intuitivamente, se logra una buena separación mediante el hiperplano que tiene la mayor distancia a los puntos de datos de entrenamiento más cercanos de cualquier clase (denominado margen funcional), ya que en general cuanto mayor es el margen, menor es el error de generalización del clasificador. \n",
        "\n",
        "El objetivo es encontrar $\\omega$ y b tal que la predicción dada por $sign(w^T\\phi(x)+b)$ sea correcta para la mayoría de las muestras\n",
        "\n",
        "$$\\underset{\\omega,b,\\zeta}{min}\\frac{1}{2}\\omega^T\\omega + C\\sum_{i=1}^{n}\\zeta_i$$\n",
        "\n",
        "sujeto a $\\quad y_i(\\omega^T\\phi(x_i)+b) \\geq 1 - \\zeta_i, \\quad \\zeta_i \\geq 0, \\quad i=1,...,n$\n",
        "\n",
        "\n",
        "Funciones kernel:\n",
        "\n",
        "* linear: $\\langle x,{x}^{'} \\rangle$\n",
        "\n",
        "* polynomial: $(\\gamma\\langle x,{x}^{'} \\rangle + r)^d$, donde **$\\gamma$** se especifica en el parámetro *gamma*, este debe ser mayor a 0. El valor de **d** se especifica en el parámetro *degree*, y **r** en *coef0*\n",
        "\n",
        "* rbf: $exp(-\\gamma{\\|x-{x}^{'}\\|}^{2})$\n",
        "\n",
        "* sigmoid: $tanh(\\gamma\\langle x,{x}^{'} \\rangle + r)$\n",
        "\n",
        "# Random Forest Classifier\n",
        "\n",
        "## sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
        "\n",
        "El bosque aleatorio es un tipo de algoritmo supervisado de aprendizaje automático basado en el aprendizaje conjunto, que soluciona el problema que tienen los árboles de decisión de no servir para reproducir escenarios predictivos what-if. Con Random Forest se puede conocer la importancia de cada variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCLbHsxgdudD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "65bf6e46-ec10-4e39-ba54-47f1ffa1ca17"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from funciones_people import pre_exploratorio, save_fig, plot_confusion_matrix, roc_multiclass, roc_auc_mc\n",
        "\n",
        "from sklearn.impute import SimpleImputer \n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from numpy.matlib import repmat\n",
        "\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.stats import multivariate_normal as mn\n",
        "\n",
        "#Modelos\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Base de datos\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-06bc556db13d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfunciones_people\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpre_exploratorio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_fig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_multiclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'funciones_people'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvRWijhududU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "886f283e-d605-4c31-b335-a6415a6290e8"
      },
      "source": [
        "#Descargando base de datos \n",
        "lfwPeople = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
        "\n",
        "#ejemplo imagen en base de datos\n",
        "print('Dimensiones base de datos: \\n',lfwPeople.images.shape)\n",
        "plt.imshow(lfwPeople.images[30,:,:],cmap='gray')\n",
        "\n",
        "print('Clases base de datos: \\n',lfwPeople['target_names'])\n",
        "print(lfwPeople.images[30,:,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4162595d0063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Descargando base de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlfwPeople\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_lfw_people\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_faces_per_person\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#ejemplo imagen en base de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dimensiones base de datos: \\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlfwPeople\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fetch_lfw_people' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvp2BBGadud2"
      },
      "source": [
        "Xdata = pd.DataFrame(lfwPeople.data)\n",
        "y = lfwPeople.target\n",
        "\n",
        "print('Tamaño dataset \\nX:', Xdata.shape)\n",
        "print('y: ', y.shape)\n",
        "\n",
        "plt.imshow(np.array(Xdata.iloc[0,:]).reshape(lfwPeople.images.shape[1],lfwPeople.images.shape[2]),cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2cXMUqPdueQ"
      },
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(Xdata, y, test_size=0.3) #30% Test\n",
        "\n",
        "Xtrain.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkCal1Midueb"
      },
      "source": [
        "#George W Bush\n",
        "\n",
        "print('tamaño y: ', y.shape)\n",
        "print(np.where(y==0)[0].shape)\n",
        "print(np.where(y==1)[0].shape)\n",
        "print(np.where(y==2)[0].shape)\n",
        "print(np.where(y==3)[0].shape)\n",
        "print(np.where(y==4)[0].shape)\n",
        "print(np.where(y==5)[0].shape)\n",
        "print(np.where(y==6)[0].shape)\n",
        "\n",
        "print('tamaño ytrain: ', ytrain.shape)\n",
        "print(np.where(ytrain==0)[0].shape)\n",
        "print(np.where(ytrain==1)[0].shape)\n",
        "print(np.where(ytrain==2)[0].shape)\n",
        "print(np.where(ytrain==3)[0].shape)\n",
        "print(np.where(ytrain==4)[0].shape)\n",
        "print(np.where(ytrain==5)[0].shape)\n",
        "print(np.where(ytrain==6)[0].shape)\n",
        "\n",
        "print('tamaño ytest: ', ytest.shape)\n",
        "print(np.where(ytest==0)[0].shape)\n",
        "print(np.where(ytest==1)[0].shape)\n",
        "print(np.where(ytest==2)[0].shape)\n",
        "print(np.where(ytest==3)[0].shape)\n",
        "print(np.where(ytest==4)[0].shape)\n",
        "print(np.where(ytest==5)[0].shape)\n",
        "print(np.where(ytest==6)[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRBeOozYduet"
      },
      "source": [
        "print('Xtrain: ',Xtrain.shape)\n",
        "img_path = 'Images/'\n",
        "\n",
        "#Se realiza reducción de dimensión con PCA y t-SNE\n",
        "pre_exploratorio(Xtrain, ytrain, img_path, 'People', lfwPeople.images.shape[1], lfwPeople.images.shape[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzeq2uBBdufE"
      },
      "source": [
        "#Clase Bayes Classifier\n",
        "\n",
        "class bayes_cla(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, priors=None, alpha=1e-6, tipo=None, tol= 1e-6):\n",
        "        self.priors = priors\n",
        "        self.alpha = alpha\n",
        "        self.tipo = tipo\n",
        "        self.tol = tol\n",
        "        return None\n",
        "\n",
        "    \n",
        "    def cov_Reg(self, X, *_):\n",
        "        mu = X.mean(axis=0)\n",
        "        dimN, dimP = X.shape\n",
        "        Xm = (X - repmat(mu,X.shape[0],1))/np.std(X)\n",
        "        if dimP <= dimN:\n",
        "            val,vec = np.linalg.eig((Xm.T).dot(Xm)+self.alpha*np.eye(dimP))\n",
        "            val = np.real(val)\n",
        "            ind = val > self.tol\n",
        "            val = val[ind]\n",
        "            vec = np.real(vec[:,ind])\n",
        "            iCov = vec.dot(np.diag(val**(-1))).dot(vec.T)\n",
        "        else:\n",
        "            val,uec = np.linalg.eig(Xm.dot(Xm.T)+self.alpha*np.eye(dimN))\n",
        "            val = np.real(val)\n",
        "            ind = val > self.tol\n",
        "            val = val[ind]\n",
        "            uec = np.real(uec[:,ind])\n",
        "            vec = (Xm.T).dot(uec).dot(np.diag(val**(-0.5)))\n",
        "            iCov = vec.dot(np.diag(val**(-1))).dot(vec.T)\n",
        "        detC = np.cumprod(val)[-1]\n",
        "        return iCov, detC\n",
        "        \n",
        "    def fit(self, X, y, *_):\n",
        "        self.classes = np.unique(y)\n",
        "        Nc = len(self.classes)\n",
        "        dimN, dimP = X.shape\n",
        "        \n",
        "        #media y covarianza por cada clase\n",
        "        \n",
        "        self.mu = np.zeros((Nc,dimP))\n",
        "        self.Cov = np.zeros((Nc,dimP,dimP))\n",
        "        self.var = np.zeros((Nc,dimP))\n",
        "        self.priors = np.zeros(Nc)\n",
        "        self.det = np.zeros(Nc)\n",
        "        \n",
        "        for i in self.classes:\n",
        "            if self.tipo == 'Naive':\n",
        "                self.priors[i] = X[y==i].shape[0]/dimN\n",
        "                self.mu[i] = np.array(X[y==i,:].mean(axis=0))\n",
        "                self.var[i] = np.array(X[y==i].var(axis=0))\n",
        "                if np.min(self.var[i]) < self.alpha:\n",
        "                    self.var[i] = self.var[i] + self.alpha\n",
        "        \n",
        "            elif self.tipo == 'LDA':\n",
        "                self.priors[i] = X[y==i].shape[0]/dimN\n",
        "                self.mu[i] = X[y==i,:].mean(axis=0)\n",
        "                #self.Cov[i], self.det[i] = self.cov_Reg(X[y==i],self.alpha,self.tol)\n",
        "                \n",
        "                self.Xm = (X - repmat(self.mu[i],X.shape[0],1))                \n",
        "                self.Cov[i] = np.cov((self.Xm[i].T).dot(self.Xm[i])+self.alpha*np.eye(dimP))\n",
        "                \n",
        "            elif self.tipo == 'QDA':\n",
        "                self.priors[i] = X[y==i].shape[0]/dimN\n",
        "                self.mu[i] = X[y==i,:].mean(axis=0) \n",
        "                #self.Cov, self.det = self.cov_Reg(X,self.alpha,self.tol)\n",
        "                self.Xm = (X - repmat(self.mu[i],X.shape[0],1))                \n",
        "                self.Cov[i] = np.cov((self.Xm[i].T).dot(self.Xm[i])+self.alpha*np.eye(dimP))\n",
        "        \n",
        "        self.det = np.linalg.det(self.Cov)\n",
        "        return self\n",
        "\n",
        "    def pdf_class_lda(self,X,mu,icovM,detC,*_):\n",
        "        dimN, dimP = X.shape\n",
        "        Xm = X - repmat(mu,X.shape[0],1)\n",
        "        pe = np.zeros(dimN)\n",
        "        for n in range(dimN):            \n",
        "            pe[n]= np.exp(-0.5*Xm[n].reshape(1,-1).dot(icovM).dot(Xm[n].reshape(-1,1))) \n",
        "        return pe\n",
        "    \n",
        "    def pdf_class_qda(self,X,mu,icovM,detC,*_):\n",
        "        dimN, dimP = X.shape\n",
        "        Xm = X - repmat(mu,X.shape[0],1)\n",
        "        pe = np.zeros(dimN)\n",
        "        for n in range(dimN):\n",
        "            \n",
        "            if n==0:\n",
        "                print('\\nfor interno\\n')\n",
        "                print('pe.shape',pe.shape)\n",
        "                print('pe[n].shape',pe[n].shape)\n",
        "                print('Xm.shape',Xm.shape)\n",
        "                print('Xm[n].shape',Xm[n].shape)\n",
        "                print('Xm[n].reshape(1,-1).shape',Xm[n].reshape(1,-1).shape)\n",
        "                print('icovM.shape',icovM.shape)\n",
        "                print('Xm[n].reshape(-1,1).shape',Xm[n].reshape(-1,1).shape)\n",
        "            \n",
        "            pe[n]= np.exp(-0.5*Xm[n].reshape(1,-1).dot(icovM).dot(Xm[n].reshape(-1,1))) \n",
        "        return pe \n",
        "\n",
        "    def predict(self, X, *_):\n",
        "        if self.tipo == 'Naive':\n",
        "            Nc = len(self.classes)\n",
        "            dimN = X.shape[0]\n",
        "            dimP = X.shape[1]\n",
        "            self.pdfT= np.zeros((Nc,dimN))\n",
        "            self.pdfs = np.zeros((Nc,dimN,dimP))\n",
        "            for i in self.classes:\n",
        "                self.pdfs[i] = np.exp((-(X-self.mu[i])**2)/(2*self.var[i]))/(np.sqrt(2*np.pi*self.var[i]))#*(self.priors[i])\n",
        "            self.pdfT= (self.pdfs).sum(axis=2)\n",
        "            return np.log(self.pdfT).argmax(axis=0)\n",
        "        \n",
        "        elif self.tipo == 'LDA':\n",
        "            Nc = len(self.classes)\n",
        "            dimN, dimP = X.shape\n",
        "            self.pdfs = np.zeros((dimN,Nc))\n",
        "            for i in self.classes:                \n",
        "                self.pdfs[:,i] = self.pdf_class_lda(X,self.mu[i],self.Cov[i],self.det[i])#*(self.priors[i])\n",
        "            self.pdfs = self.pdfs/repmat(self.pdfs.sum(axis=1).reshape(-1,1),1,self.pdfs.shape[1])\n",
        "            self.labels = self.pdfs.argmax(axis=1) \n",
        "            return self.labels\n",
        "\n",
        "        elif self.tipo == 'QDA':\n",
        "            Nc = len(self.classes)\n",
        "            dimN, dimP = X.shape\n",
        "            self.pdfs = np.zeros((dimN,Nc))\n",
        "            for i in self.classes:\n",
        "                \n",
        "                if i == 0:\n",
        "                    print('\\nfor externo\\n')\n",
        "                    print('self.pdfs.shape', self.pdfs.shape)\n",
        "                    print('self.pdfs[:,i].shape',self.pdfs[:,i].shape)\n",
        "                    print('X.shape',X.shape)\n",
        "                    print('self.mu.shape',self.mu.shape)\n",
        "                    print('self.mu[i].shape',self.mu[i].shape)\n",
        "                    print('self.Cov.shape',self.Cov.shape)\n",
        "                    print('self.det.shape',self.det.shape)\n",
        "                \n",
        "                self.pdfs[:,i] = self.pdf_class_qda(X,self.mu[i],self.Cov,self.det)#*(self.priors[i])\n",
        "            self.pdfs = self.pdfs/repmat(self.pdfs.sum(axis=1).reshape(-1,1),1,self.pdfs.shape[1])\n",
        "            self.labels = self.pdfs.argmax(axis=1) \n",
        "            return self.labels\n",
        "\n",
        "\n",
        "    def predict_proba(self, X,*_):\n",
        "        if self.tipo == 'Naive':\n",
        "            self.predict(X)\n",
        "            return self.pdfT.transpose()\n",
        "        if self.tipo == 'LDA':\n",
        "            self.predict(X)\n",
        "            return self.pdfs\n",
        "        if self.tipo == 'QDA':\n",
        "            self.predict(X)\n",
        "            return self.pdfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkH-ovZDdufZ"
      },
      "source": [
        "# SIN PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDNGX-e6dufb"
      },
      "source": [
        "Xtrain = np.array(Xtrain)\n",
        "ytrain = np.array(ytrain)\n",
        "Xtest = np.array(Xtest)\n",
        "ytest = np.array(ytest)\n",
        "\n",
        "print('ytest: ',ytest.shape)\n",
        "\n",
        "#Naive\n",
        "\n",
        "#sklearn\n",
        "naive_sk = GaussianNB()\n",
        "naive_sk.fit(Xtrain, ytrain)\n",
        "#print(naive_sk.predict(Xtest))\n",
        "\n",
        "#Clase\n",
        "naive = bayes_cla(tipo = 'Naive')\n",
        "naive.fit(Xtrain, ytrain)\n",
        "print(naive.predict(Xtest))\n",
        "np.where(naive.predict(Xtest)==ytest)[0].shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo3060uzdufn"
      },
      "source": [
        "#LDA\n",
        "\n",
        "#sklearn\n",
        "lda_sk = LinearDiscriminantAnalysis()\n",
        "lda_sk.fit(Xtrain, ytrain)\n",
        "\n",
        "#Clase\n",
        "lda = bayes_cla(tipo = 'LDA')\n",
        "lda.fit(Xtrain, ytrain)\n",
        "print(lda.predict(Xtest))\n",
        "np.where(lda.predict(Xtest)==ytest)[0].shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPLEetlDduf7"
      },
      "source": [
        "#QDA\n",
        "\n",
        "#sklearn\n",
        "qda_sk = QuadraticDiscriminantAnalysis()\n",
        "qda_sk.fit(Xtrain, ytrain)\n",
        "\n",
        "#Clase\n",
        "qda = bayes_cla(tipo = 'QDA')\n",
        "qda.fit(Xtrain, ytrain)\n",
        "print(qda.predict(Xtest))\n",
        "np.where(qda.predict(Xtest)==ytest)[0].shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcdIeTw_dugG"
      },
      "source": [
        "# Evaluando la clase\n",
        "\n",
        "ytest_naive = naive.predict(Xtest)\n",
        "acc_naive = accuracy_score(ytest, ytest_naive)\n",
        "\n",
        "ytest_lda = lda.predict(Xtest)\n",
        "acc_lda = accuracy_score(ytest, ytest_lda)\n",
        "\n",
        "ytest_qda = qda.predict(Xtest)\n",
        "acc_qda = accuracy_score(ytest, ytest_qda)\n",
        "\n",
        "plot_confusion_matrix(\n",
        "                      ytest, ytest_naive, \n",
        "                      classes = lfwPeople.target_names,\n",
        "                      normalize = True,\n",
        "                      title='ACC_Naive = %.1f %%' % (100*acc_naive)\n",
        "                      )\n",
        "\n",
        "plt.ylim([0, len(lfwPeople.target_names)+0.5])                     \n",
        "plt.show()\n",
        "\n",
        "cr_naive = classification_report(\n",
        "                           ytest, ytest_naive, \n",
        "                           labels=range(lfwPeople.target_names.shape[0]),\n",
        "                           target_names=lfwPeople.target_names\n",
        "                           )                          \n",
        "print(cr_naive)\n",
        "\n",
        "plot_confusion_matrix(\n",
        "                      ytest, ytest_lda, \n",
        "                      classes = lfwPeople.target_names,\n",
        "                      normalize = True,\n",
        "                      title='ACC_LDA = %.1f %%' % (100*acc_lda)\n",
        "                      )\n",
        "\n",
        "plt.ylim([0, len(lfwPeople.target_names)+0.5])                     \n",
        "plt.show()\n",
        "\n",
        "cr_lda = classification_report(\n",
        "                           ytest, ytest_lda, \n",
        "                           labels=range(lfwPeople.target_names.shape[0]),\n",
        "                           target_names=lfwPeople.target_names\n",
        "                           )\n",
        "print(cr_lda)\n",
        "\n",
        "plot_confusion_matrix(\n",
        "                      ytest, ytest_qda, \n",
        "                      classes = lfwPeople.target_names,\n",
        "                      normalize = True,\n",
        "                      title='ACC_QDA = %.1f %%' % (100*acc_qda)\n",
        "                      )\n",
        "\n",
        "plt.ylim([0, len(lfwPeople.target_names)+0.5])                     \n",
        "plt.show()\n",
        "\n",
        "cr_qda = classification_report(\n",
        "                           ytest, ytest_qda, \n",
        "                           labels=range(lfwPeople.target_names.shape[0]),\n",
        "                           target_names=lfwPeople.target_names\n",
        "                           )                          \n",
        "print(cr_qda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcPiDxITdugV"
      },
      "source": [
        "# CON PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqebpyFTdugc"
      },
      "source": [
        "steps_bayes = [\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', bayes_cla(tipo = 'Naive'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', bayes_cla(tipo = 'LDA'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', bayes_cla(tipo = 'QDA'))\n",
        "    ],\n",
        "    \n",
        "]\n",
        "\n",
        "parameters_bayes = [\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__alpha': [0.1, 1e-3, 1e-5, 1e-9, 1e-12],\n",
        "        'cla__tol': [0.1, 1e-3, 1e-5, 1e-9, 1e-12]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__alpha': [0.1, 1e-3, 1e-5, 1e-9, 1e-12],\n",
        "        'cla__tol': [0.1, 1e-3, 1e-5, 1e-9, 1e-12]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__alpha': [0.1, 1e-3, 1e-5, 1e-9, 1e-12],\n",
        "        'cla__tol': [0.1, 1e-3, 1e-5, 1e-9, 1e-12]\n",
        "    }\n",
        "    \n",
        "]\n",
        "\n",
        "label_models_bayes = ['Naive', 'LDA', 'QDA']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epICozK1dugj"
      },
      "source": [
        "best_model_bayes = []\n",
        "filename = 'Resultados_people_bayes'\n",
        "\n",
        "for i in range(len(steps_bayes)):\n",
        "    print('modelo %d/%d' % (i+1,len(steps_bayes)))\n",
        "    grid_search = GridSearchCV(Pipeline(steps_bayes[i]), parameters_bayes[i], n_jobs=4,cv=5,\n",
        "                                scoring='balanced_accuracy',verbose=5)\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "    print('Score: ', grid_search.best_score_)\n",
        "    \n",
        "    #mejor modelo entrenado\n",
        "    best_model_bayes += [grid_search.best_estimator_]\n",
        "    joblib.dump(best_model_bayes,filename + \".pkl\")\n",
        "\n",
        "print('Mejores modelos:\\n')\n",
        "best_model_bayes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14nnmtwIdugv"
      },
      "source": [
        "my_model_loaded_bayes = joblib.load(filename + \".pkl\")\n",
        "\n",
        "for i in range(len(my_model_loaded_bayes)):\n",
        "    print('Evaluando modelo %d/%d' % (i+1,len(my_model_loaded_bayes)))\n",
        "\n",
        "    ytest_e = my_model_loaded_bayes[i].predict(Xtest)\n",
        "    acc = accuracy_score(ytest, ytest_e)\n",
        "  \n",
        "    plot_confusion_matrix(\n",
        "                          ytest, ytest_e, \n",
        "                          classes = lfwPeople.target_names,\n",
        "                          normalize = True,\n",
        "                          title='ACC = %.1f %%' % (100*acc)\n",
        "                          )\n",
        "    plt.ylim([0, len(lfwPeople.target_names)+0.5])\n",
        "    save_fig(img_path, label_models_bayes[i])                      \n",
        "    plt.show()\n",
        "    \n",
        "    cr = classification_report(\n",
        "                               ytest, ytest_e, \n",
        "                               labels=range(lfwPeople.target_names.shape[0]),\n",
        "                               target_names=lfwPeople.target_names\n",
        "                               )                          \n",
        "    #support = #muestras en la clase estudiada\n",
        "    print(cr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ePY8dODdug7"
      },
      "source": [
        "# Train sk-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTM3DpQbduhP"
      },
      "source": [
        "steps = [\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', GaussianNB())\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', SGDClassifier(loss='hinge'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', SGDClassifier(loss='log'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', SGDClassifier(loss='modified_huber'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', LinearDiscriminantAnalysis(solver='svd'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', LinearDiscriminantAnalysis(solver='eigen',\n",
        "                                          shrinkage='auto'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', QuadraticDiscriminantAnalysis())\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', KNeighborsClassifier(algorithm='auto'))\n",
        "    ],\n",
        "        \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', KNeighborsClassifier(algorithm='ball_tree'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', LogisticRegression(solver='lbfgs'))\n",
        "    ],\n",
        "                                     \n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', LogisticRegression(solver='newton-cg'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', SVC(kernel='rbf'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', SVC(kernel='sigmoid',\n",
        "                   decision_function_shape='ovo'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', RandomForestClassifier(max_features='auto'))\n",
        "    ],\n",
        "    \n",
        "    [\n",
        "        ('rep', PCA()),\n",
        "        ('cla', RandomForestClassifier(max_features='log2'))\n",
        "    ],\n",
        "    \n",
        "]\n",
        "\n",
        "parameters = [\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9]        \n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__alpha': [0.0001, 0.1, 0.5, 1]    \n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__alpha': [0.0001, 0.1, 0.5, 1]    \n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__alpha': [0.0001, 0.1, 0.5, 1],\n",
        "        'cla__epsilon': [0.1, 0.5, 0.9]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9]        \n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9]        \n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__reg_param': [0, 0.5, 1]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__n_neighbors': [0.1, 1, 3, 5]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__n_neighbors': [0.1, 1, 3, 5]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__C': [0.1, 0.5, 1]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__C': [0.1, 0.5, 1]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__C': [0.1, 0.5, 1]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__n_estimators': [20, 100, 300, 500]\n",
        "    },\n",
        "    \n",
        "    {\n",
        "        'rep__n_components': [0.1, 0.5, 0.9],\n",
        "        'cla__n_estimators': [20, 100, 300, 500]\n",
        "    },\n",
        "    \n",
        "]\n",
        "\n",
        "label_models = [\n",
        "    \n",
        "    'Naive',\n",
        "    'SGD_hinge',\n",
        "    'SGD_log',\n",
        "    'SGD_huber',\n",
        "    'LDA_svd',\n",
        "    'LDA_eigen',\n",
        "    'QDA',\n",
        "    'KN_auto',\n",
        "    'KN_ballTree',\n",
        "    'Log_lbfgs',\n",
        "    'Log_newton',\n",
        "    'SVC_rbf',\n",
        "    'RandFo_auto',\n",
        "    'RandFo_log2'    \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJST78sfduhb"
      },
      "source": [
        "best_model = []\n",
        "filename = 'Resultados_people'\n",
        "\n",
        "for i in range(len(steps)):\n",
        "    print('modelo %d/%d' % (i+1,len(steps)))\n",
        "    grid_search = GridSearchCV(Pipeline(steps[i]), parameters[i], n_jobs=4,cv=5,\n",
        "                                scoring='balanced_accuracy',verbose=5)\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "    print('Score: ', grid_search.best_score_)\n",
        "    \n",
        "    #mejor modelo entrenado\n",
        "    best_model += [grid_search.best_estimator_]\n",
        "    joblib.dump(best_model,filename + \".pkl\")\n",
        "\n",
        "print('Mejores modelos:\\n')\n",
        "best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQO_88rvduhf"
      },
      "source": [
        "my_model_loaded = joblib.load(filename + \".pkl\")\n",
        "\n",
        "for i in range(len(my_model_loaded)):\n",
        "    print('Evaluando modelo %d/%d' % (i+1,len(my_model_loaded)))\n",
        "\n",
        "    ytest_e = my_model_loaded[i].predict(Xtest)\n",
        "    acc = accuracy_score(ytest, ytest_e)\n",
        "  \n",
        "    plot_confusion_matrix(\n",
        "                          ytest, ytest_e, \n",
        "                          classes = lfwPeople.target_names,\n",
        "                          normalize = True,\n",
        "                          title='ACC = %.1f %%' % (100*acc)\n",
        "                          )\n",
        "    plt.ylim([0, len(lfwPeople.target_names)+0.5])\n",
        "    save_fig(img_path, label_models[i])                      \n",
        "    plt.show()\n",
        "    \n",
        "    cr = classification_report(\n",
        "                               ytest, ytest_e, \n",
        "                               labels=range(lfwPeople.target_names.shape[0]),\n",
        "                               target_names=lfwPeople.target_names\n",
        "                               )                          \n",
        "    #support = #muestras en la clase estudiada\n",
        "    print(cr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_0E54rWduhp"
      },
      "source": [
        "ytrain_b = label_binarize(ytrain, classes=range(lfwPeople.target_names.shape[0]))\n",
        "ytest_b = label_binarize(ytest, classes=range(lfwPeople.target_names.shape[0]))\n",
        "\n",
        "for i in range(len(my_model_loaded)):\n",
        "    print('Evaluando ROC modelo %d/%d' % (i+1,len(my_model_loaded)))\n",
        "    try: #debe calcularse la funcion de decision o el posterior de la probabilidad\n",
        "        ytest_score = my_model_loaded[i].decision_function(Xtest)\n",
        "    except:\n",
        "        ytest_score = my_model_loaded[i].predict_proba(Xtest)\n",
        "    roc_auc, fpr, tpr, n_classes = roc_multiclass(ytest_b,ytest_score)\n",
        "    roc_auc_mc(roc_auc,fpr,tpr,n_classes,'ROC curve ' + label_models[i],img_path)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "021so3anduhy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}